{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "gulls-guide.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNmTKPrYsEwDhjjsYRyxY9m"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4l_IP8mUHKSs",
        "colab_type": "text"
      },
      "source": [
        "#Convolutional Neural Network for identifying 3 most commonly mistaken species of gulls in Poland - *Larus argentatus*, *Larus cachinnans* and *Larus michachellis*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Scq6n-D5LbfJ",
        "colab_type": "text"
      },
      "source": [
        "<table align=\"center\">\n",
        "  <td align=\"center\"><a target=\"_blank\" href=\"https://colab.research.google.com/github/bazylip/gulls-ai-guide/blob/master/gulls_guide.ipynb\">\n",
        "        <img src=\"http://introtodeeplearning.com/images/colab/colab.png?v2.0\"  style=\"padding-bottom:5px;\" />Run in Google Colab</a></td>\n",
        "  <td align=\"center\"><a target=\"_blank\" href=\"https://github.com/bazylip/gulls-ai-guide/blob/dev/gulls_guide.ipynb\">\n",
        "        <img src=\"http://introtodeeplearning.com/images/colab/github.png\"  height=\"70px\" style=\"padding-bottom:5px;\"  />View Source on GitHub</a></td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_P-8tEUIHE4k",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "f4e773c6-18db-42a3-cae0-f78370f1fe1d"
      },
      "source": [
        "# Import Tensorflow 2.0\n",
        "%tensorflow_version 2.x\n",
        "import tensorflow as tf \n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import random\n",
        "import re\n",
        "from PIL import Image, ImageOps, ImageFile\n",
        "from io import BytesIO\n",
        "from tqdm import tqdm\n",
        "from bs4 import BeautifulSoup\n",
        "from google.colab import files, drive\n",
        "import requests\n",
        "import collections\n",
        "\n",
        "# Check that we are using a GPU, if not switch runtimes\n",
        "#   using Runtime > Change Runtime Type > GPU\n",
        "assert len(tf.config.list_physical_devices('GPU')) > 0\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "DOWNLOAD_FILES = False\n",
        "IMG_SIZE = 400"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wah-T8cgJeP8",
        "colab_type": "text"
      },
      "source": [
        "Download images from [Gull Research website](http://www.gull-research.org/)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9LzQeNBKJdtj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if DOWNLOAD_FILES:\n",
        "  urls = ['http://www.gull-research.org/hg/hg1cy/', 'http://www.gull-research.org/hg/hg2cy/',\n",
        "          'http://www.gull-research.org/hg/hg3cy/', 'http://www.gull-research.org/hg/hg4cy/',\n",
        "          'http://www.gull-research.org/hg/hg5cy/']\n",
        "  image_urls = collections.defaultdict(list)\n",
        "\n",
        "  def list_files(url, ext='html'):\n",
        "      page = requests.get(url).text\n",
        "      soup = BeautifulSoup(page, 'html.parser')\n",
        "      return [url + node.get('href') for node in soup.find_all('a') if node.get('href').endswith(ext)]\n",
        "    \n",
        "  def get_all_image_urls(site):\n",
        "      response = requests.get(site)\n",
        "\n",
        "      soup = BeautifulSoup(response.text, 'html.parser')\n",
        "      img_tags = soup.find_all('img')\n",
        "\n",
        "      urls = [img['src'] for img in img_tags]\n",
        "      image_urls = []\n",
        "      for url in urls:\n",
        "          filename = re.search(r'/([\\w_-]+[.](jpg|png|JPG|PNG))$', url)\n",
        "          if not filename:\n",
        "              continue\n",
        "          if 'http' not in url:\n",
        "              # sometimes an image source can be relative \n",
        "              # if it is provide the base url which also happens \n",
        "              # to be the site variable atm.\n",
        "              site_level_below = '/'.join(site.split('/')[:-2])\n",
        "              url_with_no_dots = '/'.join(url.split('/')[1:]) \n",
        "              url = f\"{site_level_below}/{url_with_no_dots}\"\n",
        "          image_urls.append(url)\n",
        "      return image_urls\n",
        "\n",
        "  for age, url in enumerate(urls):\n",
        "    for site in list_files(url):\n",
        "      urls_from_site = get_all_image_urls(site)\n",
        "      \n",
        "      if urls_from_site is not None:\n",
        "        for image_url in urls_from_site:\n",
        "          image_urls[age+1].append(image_url)\n",
        "\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aOxpw2EVeUmw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if DOWNLOAD_FILES:\n",
        "  for age in image_urls.keys():\n",
        "    print(f\"Images for age {age}: {len(image_urls[age])}\")"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zv6NwXkMkkpB",
        "colab_type": "text"
      },
      "source": [
        "Download the images and turn them into np.array"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "44_9KX-flEZE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if DOWNLOAD_FILES:\n",
        "\n",
        "  images_and_labels = []\n",
        "\n",
        "  ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
        "\n",
        "  for age in image_urls.keys():\n",
        "    for url in image_urls[age]:\n",
        "      try:\n",
        "        response = requests.get(url)\n",
        "        img = Image.open(BytesIO(response.content))\n",
        "        square_size = max(img.size)\n",
        "\n",
        "        img_square = ImageOps.fit(img, (square_size, square_size), Image.ANTIALIAS)\n",
        "        img_resized = img_square.resize((IMG_SIZE, IMG_SIZE), Image.ANTIALIAS)\n",
        "\n",
        "        img_array = np.asarray(img_resized)\n",
        "        images_and_labels.append((img_array, age))\n",
        "      except:\n",
        "        pass\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q2cnzSFMwEhf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if DOWNLOAD_FILES:\n",
        "  for tup in images_and_labels:\n",
        "    if tup[0].shape != (400,400,3):\n",
        "      images_and_labels.pop(images_and_labels.index(tup))"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h4SCNY3ydiTr",
        "colab_type": "text"
      },
      "source": [
        "Now divide the images and labels into train set and test set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n2JUgRL-cC6E",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "9426d836-8b7e-4025-870f-5550d9175368"
      },
      "source": [
        "if DOWNLOAD_FILES:\n",
        "  random.shuffle(images_and_labels)\n",
        "  train_set_size = 1000\n",
        "\n",
        "  train_images = np.asarray([tup[0] for tup in images_and_labels][:-train_set_size])\n",
        "  train_labels = np.asarray([tup[1]-1 for tup in images_and_labels][:-train_set_size])\n",
        "\n",
        "  test_images = np.asarray([tup[0] for tup in images_and_labels][-train_set_size:])\n",
        "  test_labels = np.asarray([tup[1]-1 for tup in images_and_labels][-train_set_size:])\n",
        "\n",
        "  np_arrays = [train_images, train_labels, test_images, test_labels]\n",
        "  np_arrays_names = [\"train_images\", \"train_labels\", \"test_images\", \"test_labels\"]\n",
        "  for np_array, np_array_name in zip(np_arrays, np_arrays_names):\n",
        "    with open(f\"/content/drive/My Drive/{np_array_name}.npy\", \"w+b\") as f:\n",
        "      np.save(f, np_array)\n",
        "else:\n",
        "  def load_binaries():\n",
        "    file_names = [\"train_images\", \"train_labels\", \"test_images\", \"test_labels\"]\n",
        "    nd_arrays = []\n",
        "\n",
        "    for file_name in file_names:\n",
        "      with open(f\"/content/drive/My Drive/{file_name}.npy\", \"rb\") as f:\n",
        "        nd_arrays.append(np.load(f))\n",
        "\n",
        "    return (nd_arrays[0], nd_arrays[1]), (nd_arrays[2], nd_arrays[3])\n",
        "\n",
        "  (train_images, train_labels), (test_images, test_labels) = load_binaries()      \n",
        "\n",
        "\n",
        "print(f\"Length of training set: {len(train_images)}\")\n",
        "print(f\"Length of test set: {len(test_images)}\")"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Length of training set: 6420\n",
            "Length of test set: 1000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F9MzRw88qH8T",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "outputId": "324a305d-05e6-4b03-f7fb-f6aabbd9692c"
      },
      "source": [
        "filtered_images = np.array([])\n",
        "\n",
        "count = 0\n",
        "for image, label in zip(train_images, train_labels):\n",
        "  if label == 0:\n",
        "    filtered_images = np.append(filtered_images, image)\n",
        "  count+=1\n",
        "  if count == 300:\n",
        "    break\n",
        "np.remove(filtered_images, 0)\n",
        "\n",
        "train_images, train_labels = filtered_images, np.zeros(len(filtered_images))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-93da26d3182f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mcount\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m300\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mremove\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_images\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mtrain_images\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfiltered_images\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_images\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: module 'numpy' has no attribute 'remove'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RPcD3K_IfaXa",
        "colab_type": "text"
      },
      "source": [
        "Let's see some example pictures from the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DjWvH-GTffmX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def numpy2pil(np_array):\n",
        "    assert_msg = 'Input shall be a HxWx3 ndarray'\n",
        "    assert isinstance(np_array, np.ndarray), assert_msg\n",
        "    assert len(np_array.shape) == 3, assert_msg\n",
        "    assert np_array.shape[2] == 3, assert_msg\n",
        "\n",
        "    img = Image.fromarray(np_array, 'RGB')\n",
        "    return img\n",
        "\n",
        "display(numpy2pil(train_images[1]))\n",
        "print(train_labels[1]+1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jK9LYgj2hq64",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.figure(figsize=(10,10))\n",
        "random_inds = np.random.choice(4000,9)\n",
        "for i in range(9):\n",
        "    plt.subplot(3,3,i+1)\n",
        "    plt.xticks([])\n",
        "    plt.yticks([])\n",
        "    plt.grid(False)\n",
        "    image_ind = random_inds[i]\n",
        "    plt.imshow(np.squeeze(train_images[image_ind]), cmap=plt.cm.binary)\n",
        "    plt.xlabel(f\"Age {train_labels[image_ind]+1}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gYRiX9vbjg7Z",
        "colab_type": "text"
      },
      "source": [
        "Let's get to work and build the network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y7AHCi_ejjJC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "LEARNING_RATE = 1e-3\n",
        "\n",
        "def build_cnn_model():\n",
        "    cnn_model = tf.keras.Sequential([\n",
        "        tf.keras.layers.experimental.preprocessing.Rescaling(1./255, input_shape=(IMG_SIZE, IMG_SIZE, 3)),\n",
        "\n",
        "        tf.keras.layers.Conv2D(filters=32, kernel_size=3, activation='relu'), \n",
        "        tf.keras.layers.MaxPool2D(pool_size=(2,2)),\n",
        "        tf.keras.layers.Conv2D(filters=64, kernel_size=3, activation='relu'),\n",
        "        tf.keras.layers.MaxPool2D(pool_size=(2,2)),\n",
        "        tf.keras.layers.Conv2D(filters=128, kernel_size=3, activation='relu'),\n",
        "        tf.keras.layers.MaxPool2D(pool_size=(2,2)),\n",
        "\n",
        "        tf.keras.layers.Flatten(),\n",
        "\n",
        "        tf.keras.layers.Dense(128, activation=tf.nn.relu),\n",
        "        tf.keras.layers.Dense(5, activation=tf.nn.sigmoid)\n",
        "    ])\n",
        "    \n",
        "    return cnn_model\n",
        "  \n",
        "cnn_model = build_cnn_model()\n",
        "\n",
        "cnn_model.compile(optimizer=tf.keras.optimizers.SGD(learning_rate=LEARNING_RATE), \n",
        "                  loss='sparse_categorical_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "print(cnn_model.summary())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B5EH1qsrlzpI",
        "colab_type": "text"
      },
      "source": [
        "Train the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "frnDgwGil1KD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BATCH_SIZE = 128\n",
        "EPOCHS = 5\n",
        "\n",
        "history = cnn_model.fit(train_images, train_labels, batch_size=BATCH_SIZE, epochs=EPOCHS)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aJtrOzedwjhd",
        "colab_type": "text"
      },
      "source": [
        "Check accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ri-9Ng_LwiUV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_loss, test_acc = cnn_model.evaluate(test_images, test_labels)\n",
        "\n",
        "print('Test accuracy:', test_acc)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JPunIvT6uWlu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predictions = cnn_model.predict(test_images)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rbds_OyDwlAx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.figure(figsize=(10,10))\n",
        "random_inds = np.random.choice(500,9)\n",
        "for i in range(9):\n",
        "    plt.subplot(3,3,i+1)\n",
        "    plt.xticks([])\n",
        "    plt.yticks([])\n",
        "    plt.grid(False)\n",
        "    image_ind = random_inds[i]\n",
        "    plt.imshow(np.squeeze(test_images[image_ind]), cmap=plt.cm.binary)\n",
        "    plt.xlabel(f\"Age: {test_labels[image_ind]+1}, predicted: {predictions[image_ind].argmax()+1}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8B5ctcGExpa4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predictions_argmax = [pred.argmax() for pred in predictions]\n",
        "unique, counts = np.unique(predictions_argmax, return_counts=True)\n",
        "dict(zip(unique, counts))\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}